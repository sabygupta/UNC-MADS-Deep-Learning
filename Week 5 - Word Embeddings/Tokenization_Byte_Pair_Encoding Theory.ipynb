{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzqzWdz_1QOf"
      },
      "source": [
        "## Reading in a short story as text sample into Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ckGku31QOi"
      },
      "source": [
        "## Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hzZOdxt1QOl"
      },
      "source": [
        "Our goal is to tokenize documents into individual words and special\n",
        "characters that we can then turn into embeddings for LLM training.\n",
        "Note that it's common to process millions of articles and hundreds of thousands of\n",
        "books -- many gigabytes of text -- when working with LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F_TjALg1QOm"
      },
      "source": [
        "Using some simple example text, we can use the re.split command with the following\n",
        "syntax to split a text on whitespace characters:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Id61QmEw1QOn",
        "outputId": "00a5fa01-a619-4b85-da6d-036609aec429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7B8Argp1QOn"
      },
      "source": [
        "The result is a list of individual words, whitespaces, and punctuation characters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX_EFLcD1QOn"
      },
      "source": [
        "Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods\n",
        "([,.]):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rLGvrtii1QOn",
        "outputId": "272909ab-3e06-4fd6-ac1f-4cfffb8239fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJFIF_y1QOo"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
        "can remove these redundant characters safely as follows:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MrPeU0ME1QOo",
        "outputId": "95690d6a-c0b1-43b9-bde2-dc748c3da436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3o-Ushd1QOo"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "REMOVING WHITESPACES OR NOT\n",
        "\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as\n",
        "separate characters or just remove them depends on our application and its\n",
        "requirements. Removing whitespaces reduces the memory and computing\n",
        "requirements. However, keeping whitespaces can be useful if we train models that\n",
        "are sensitive to the exact structure of the text (for example, Python code, which is\n",
        "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
        "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
        "that includes whitespaces.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtHfODs91QOq"
      },
      "source": [
        "## Step 2: Creating Token IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_gxMkn_1QOq"
      },
      "source": [
        "In the previous section, we tokenized document. Let's now create a list of all unique tokens and sort\n",
        "them alphabetically to determine the vocabulary size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9plLsfp1QOq",
        "outputId": "b9fc8cbe-be4d-49fd-ae45-8ac64b9cef26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Words:  [',', '.', 'Hello', 'This', 'a', 'is', 'test', 'world']\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(result))\n",
        "print (\"Unique Words: \", all_words)\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SF3o3z41QOr"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
        "vocabulary and print its first 51 entries for illustration purposes:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OumhtLUE1QOr"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R75M_hyh1QOr",
        "outputId": "2596c021-1b38-490d-947f-831e160bce04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(',', 0)\n",
            "('.', 1)\n",
            "('Hello', 2)\n",
            "('This', 3)\n",
            "('a', 4)\n",
            "('is', 5)\n",
            "('test', 6)\n",
            "('world', 7)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5flgaju1QOr"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "As we can see, based on the output above, the dictionary contains individual tokens\n",
        "associated with unique integer labels.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSvqJbk01QOs"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "    \n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR2uCNUE1QOy"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "So far, so good. Now tokenize: text = \"Hello, do you like tea?\". This will throw an error as 'Hello' not contained in the vocabulary..\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz05iKIF1QOy"
      },
      "source": [
        "### ADDING SPECIAL CONTEXT TOKENS\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>\n",
        "\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxHKL-_01QO1"
      },
      "source": [
        "Depending on the LLM, some researchers also consider additional special tokens such\n",
        "as the following:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to <|endoftext|>. For instance, when combining two different\n",
        "Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "[PAD] token, up to the length of the longest text in the batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mhqVgHb1QO1"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "above but only uses an <|endoftext|> token for simplicity.\n",
        "\n",
        "Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
        "down words into subword units\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHM4h2fs1QO1"
      },
      "source": [
        "### BYTE PAIR ENCODING (BPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHVg-kET1QO1"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We implemented a simple tokenization scheme in the previous sections for illustration\n",
        "purposes.\n",
        "\n",
        "This section covers a more sophisticated tokenization scheme based on a concept\n",
        "called byte pair encoding (BPE).\n",
        "\n",
        "The BPE tokenizer covered in this section was used to train\n",
        "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "3 types of tokenizers:\n",
        "\n",
        "1. word based\n",
        "    * (-) missing vocab (out of vocab) words \n",
        "    * (-) similar words like singular/plural terms\n",
        "2. sub-word based - break into smaller meaningful subwords\n",
        "3. character based - individual characters with small vocab size like english ~256\n",
        "    * (+) memory efficient \n",
        "    * (-) semantic knowledge missing\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is BPE?\n",
        "\n",
        "BPE iteratively merges the most frequent pair of characters or character sequences to create a vocabulary of subword units.\n",
        "\n",
        "## Step-by-Step Example\n",
        "\n",
        "Let's say we have a small corpus:\n",
        "\n",
        "### **Initial Corpus**\n",
        "```\n",
        "low low low low low\n",
        "lower lower lower\n",
        "newest newest newest newest newest newest\n",
        "widest widest widest\n",
        "```\n",
        "\n",
        "### **Step 1: Start with Characters**\n",
        "\n",
        "Split everything into individual characters (with end-of-word marker `</w>`):\n",
        "\n",
        "```\n",
        "l o w </w>         (appears 5 times)\n",
        "l o w e r </w>     (appears 3 times)  \n",
        "n e w e s t </w>   (appears 6 times)\n",
        "w i d e s t </w>   (appears 3 times)\n",
        "```\n",
        "\n",
        "**Initial vocabulary:** \n",
        "```\n",
        "{l, o, w, e, r, n, i, d, s, t, </w>}  # 11 tokens\n",
        "```\n",
        "\n",
        "### **Step 2: Find Most Frequent Pair**\n",
        "\n",
        "Count all adjacent character pairs:\n",
        "\n",
        "```\n",
        "Pair        Frequency\n",
        "(e, s)      6+3 = 9     ← Most frequent!\n",
        "(s, t)      6+3 = 9     ← Tie!\n",
        "(l, o)      5+3 = 8\n",
        "(o, w)      5+3 = 8\n",
        "(w, </w>)   5\n",
        "...\n",
        "```\n",
        "\n",
        "Let's merge `(e, s)` first.\n",
        "\n",
        "### **Step 3: Merge Most Frequent Pair**\n",
        "\n",
        "Replace all `e s` with `es`:\n",
        "\n",
        "```\n",
        "l o w </w>\n",
        "l o w e r </w>\n",
        "n e w es t </w>      ← merged!\n",
        "w i d es t </w>      ← merged!\n",
        "```\n",
        "\n",
        "**Vocabulary now:**\n",
        "```\n",
        "{l, o, w, e, r, n, i, d, s, t, </w>, es}  # 12 tokens\n",
        "```\n",
        "\n",
        "### **Step 4: Repeat**\n",
        "\n",
        "Find next most frequent pair:\n",
        "\n",
        "```\n",
        "Pair           Frequency\n",
        "(es, t)        6+3 = 9    ← Most frequent!\n",
        "(l, o)         5+3 = 8\n",
        "(o, w)         5+3 = 8\n",
        "...\n",
        "```\n",
        "\n",
        "Merge `(es, t)` → `est`:\n",
        "\n",
        "```\n",
        "l o w </w>\n",
        "l o w e r </w>\n",
        "n e w est </w>       ← merged!\n",
        "w i d est </w>       ← merged!\n",
        "```\n",
        "\n",
        "**Vocabulary now:**\n",
        "```\n",
        "{l, o, w, e, r, n, i, d, s, t, </w>, es, est}  # 13 tokens\n",
        "```\n",
        "\n",
        "### **Step 5: Continue Merging**\n",
        "\n",
        "Keep going until you reach desired vocabulary size:\n",
        "\n",
        "**Iteration 3:** Merge `(est, </w>)` → `est</w>`\n",
        "```\n",
        "l o w </w>\n",
        "l o w e r </w>\n",
        "n e w est</w>\n",
        "w i d est</w>\n",
        "```\n",
        "\n",
        "**Iteration 4:** Merge `(l, o)` → `lo`\n",
        "```\n",
        "lo w </w>\n",
        "lo w e r </w>\n",
        "n e w est</w>\n",
        "w i d est</w>\n",
        "```\n",
        "\n",
        "**Iteration 5:** Merge `(lo, w)` → `low`\n",
        "```\n",
        "low </w>\n",
        "low e r </w>\n",
        "n e w est</w>\n",
        "w i d est</w>\n",
        "```\n",
        "\n",
        "And so on...\n",
        "\n",
        "## Final Result\n",
        "\n",
        "After many iterations, you might end up with:\n",
        "\n",
        "**Final Vocabulary:**\n",
        "```\n",
        "{\n",
        "  # Characters\n",
        "  l, o, w, e, r, n, i, d, s, t,\n",
        "  \n",
        "  # Subwords  \n",
        "  lo, ow, low, low</w>, er, er</w>,\n",
        "  es, est, est</w>, new, newest</w>,\n",
        "  wi, id, wide, widest</w>\n",
        "}\n",
        "```\n",
        "\n",
        "## How to Tokenize New Words\n",
        "\n",
        "Now you can tokenize new words using learned merges:\n",
        "\n",
        "**Example: \"lowest\"** (unseen word!)\n",
        "\n",
        "```\n",
        "Step 1: l o w e s t </w>\n",
        "Step 2: lo w e s t </w>          (merge l+o)\n",
        "Step 3: low e s t </w>           (merge lo+w)\n",
        "Step 4: low es t </w>            (merge e+s)\n",
        "Step 5: low est </w>             (merge es+t)\n",
        "Final:  [low, est</w>]\n",
        "```\n",
        "\n",
        "Even though \"lowest\" wasn't in training, BPE can tokenize it using learned subwords!\n",
        "\n",
        "## BPE vs FastText vs Word2Vec\n",
        "\n",
        "| Method | Unit | Example: \"running\" |\n",
        "|--------|------|-------------------|\n",
        "| **Word2Vec** | Whole words | `[\"running\"]` |\n",
        "| **FastText** | Character n-grams | `[\"<ru\", \"run\", \"unn\", \"nni\", \"nin\", \"ing\", \"ng>\"]` |\n",
        "| **BPE** | Learned subwords | `[\"runn\", \"ing\"]` |\n",
        "\n",
        "## Real-World BPE Example (GPT)\n",
        "\n",
        "Modern models like GPT use BPE with ~50k vocabulary:\n",
        "\n",
        "```python\n",
        "# How GPT might tokenize\n",
        "\"unhappiness\" → [\"un\", \"happiness\"]\n",
        "\"running\"     → [\"run\", \"ning\"]\n",
        "\"COVID-19\"    → [\"COVID\", \"-\", \"19\"]\n",
        "\"hyperglycemia\" → [\"hyper\", \"gly\", \"cemia\"]\n",
        "```\n",
        "## Why BPE is Important\n",
        "\n",
        "**Advantages:**\n",
        "- ✓ Handles rare/unseen words (unlike Word2Vec)\n",
        "- ✓ Fixed vocabulary size\n",
        "- ✓ Data-driven (learns from corpus)\n",
        "- ✓ Language-agnostic\n",
        "\n",
        "**Used in:**\n",
        "- GPT (OpenAI)\n",
        "- BERT variants\n",
        "- Most modern LLMs\n",
        "- Machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hzgQ0aH1QO1"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
        "open-source library called tiktoken (https://github.com/openai/tiktoken).\n",
        "\n",
        "This library implements\n",
        "the BPE algorithm very efficiently based on source code in Rust.\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".tf_env (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
