{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9ea8f",
   "metadata": {},
   "source": [
    "Assuming a standard linear regression model with independent normal errors, the ordinary least squares (OLS) estimator of $\\beta$ is exactly the maximum likelihood estimator (MLE) because maximizing the Gaussian likelihood is equivalent to minimizing the sum of squared residuals.\n",
    "\n",
    "## Model and likelihood\n",
    "\n",
    "Consider the linear model\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\varepsilon,\n",
    "$$\n",
    "\n",
    "where $Y \\in \\mathbb{R}^n$, $X$ is an $n \\times p$ matrix of predictors, $\\beta \\in \\mathbb{R}^p$ is the parameter vector, and\n",
    "\n",
    "$$\n",
    "\\varepsilon \\sim N(0, \\sigma^2 I_n),\n",
    "$$\n",
    "\n",
    "with $\\sigma^2 > 0$ and $I_n$ the $n \\times n$ identity matrix.\n",
    "\n",
    "Conditional on $X$, the distribution of $Y$ is multivariate normal:\n",
    "\n",
    "$$\n",
    "Y \\mid X \\sim N(X\\beta, \\sigma^2 I_n).\n",
    "$$\n",
    "\n",
    "The joint density (likelihood) of $Y$ given $\\beta, \\sigma^2$ is\n",
    "\n",
    "$$\n",
    "L(\\beta, \\sigma^2 \\mid y)\n",
    "= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2\\sigma^2}(y - X\\beta)^{\\top}(y - X\\beta)\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Taking logs gives the log-likelihood (up to constants not depending on $\\beta$):\n",
    "\n",
    "$$\n",
    "\\ell(\\beta, \\sigma^2 \\mid y)\n",
    "= -\\frac{n}{2}\\log(2\\pi)\n",
    "- \\frac{n}{2}\\log(\\sigma^2)\n",
    "- \\frac{1}{2\\sigma^2}(y - X\\beta)^{\\top}(y - X\\beta).\n",
    "$$\n",
    "\n",
    "## Equivalence to least squares\n",
    "\n",
    "For a fixed $\\sigma^2$, maximizing $\\ell(\\beta, \\sigma^2 \\mid y)$ with respect to $\\beta$ is the same as minimizing the quadratic term\n",
    "\n",
    "$$\n",
    "Q(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta),\n",
    "$$\n",
    "\n",
    "because the other terms in $\\ell$ do not depend on $\\beta$ and $-\\frac{1}{2\\sigma^2}$ is a negative constant.\n",
    "\n",
    "But minimizing $Q(\\beta)$ is precisely the ordinary least squares criterion:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{OLS}} \n",
    "= \\arg\\min_{\\beta} (y - X\\beta)^{\\top}(y - X\\beta).\n",
    "$$\n",
    "\n",
    "To find the minimizer, differentiate $Q(\\beta)$ and set to zero:\n",
    "\n",
    "$$\n",
    "Q(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta)\n",
    "= y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q(\\beta)}{\\partial \\beta}\n",
    "= -2X^{\\top}y + 2X^{\\top}X\\beta.\n",
    "$$\n",
    "\n",
    "Set the gradient to zero:\n",
    "\n",
    "$$\n",
    "-2X^{\\top}y + 2X^{\\top}X\\hat{\\beta} = 0\n",
    "\\quad \\Longrightarrow \\quad\n",
    "X^{\\top}X\\hat{\\beta} = X^{\\top}y.\n",
    "$$\n",
    "\n",
    "If $X^{\\top}X$ is invertible,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{MLE}} \n",
    "= \\hat{\\beta}_{\\text{OLS}}\n",
    "= (X^{\\top}X)^{-1}X^{\\top}y.\n",
    "$$\n",
    "\n",
    "Thus, under the normal-error assumption, the least squares estimator is exactly the maximum likelihood estimator for $\\beta$ in the linear regression model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
